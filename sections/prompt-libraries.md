# Prompt-libraries Repositories

## [Openwebui Prompt Library](https://github.com/danielrosehill/OpenWebUI-Prompt-Library)
A selection of prompts designed for use with Open Web UI (rather than conventional prompts, they're more useful for steering existing conversations)

## [Roocode Mode Prompts](https://github.com/danielrosehill/RooCode-Mode-Prompts)
Some custom mode-setting system prompts for Roo Code

## [Prompt And Output Separator](https://github.com/danielrosehill/Prompt-And-Output-Separator)
Streamlit-deployed app which attempts to separate prompts and outputs from continuous text files

## [Prompt Puncutation Experiment](https://github.com/danielrosehill/Prompt-Puncutation-Experiment)
Experimenting to test the effect of puncutation in prompts on inference quality

## [Fun Large Language Model Data Prompts](https://github.com/danielrosehill/Fun-LLM-Data-Prompts)
As prompting fro data analysis & vis is quite specific, a small side collection of prompts for this purpose

## [Large Language Model System Prompts](https://github.com/danielrosehill/LLM-System-Prompts)
A few system prompts for LLMs intended to improve the format and utility of outputs. Written primarily for LibreChat but should be reasonably frontend-agnostic

## [Prompt Builder](https://github.com/danielrosehill/Prompt-Builder)
Work in progress. Assemble prompts based upon templated forms in the browser. 

## [Prompt Engineering Resources](https://github.com/danielrosehill/Prompt-Engineering-Resources)
Lists of resources and tools for prompt engineering and evaluation

## [My Principles Of Prompting](https://github.com/danielrosehill/My-Principles-Of-Prompting)
A small and opinionated repository of my thoughts about prompting LLMs to maximise their utility for specific use-cases

## [Prompts And Outputs](https://github.com/danielrosehill/Prompts-And-Outputs)
A vault of over 2,000 prompts and outputs derived from interactions with various large language models (LLMs)

## [Large Language Model Evaluation Prompts](https://github.com/danielrosehill/LLM-Evaluation-Prompts)
A few prompts that I am storing in a repo for the purpose of running controlled experiments comparing and benchmarking different LLMs for defined use-cases

## [Awesome Large Language Model Prompt Libraries](https://github.com/danielrosehill/Awesome-LLM-Prompt-Libraries)
An index of prompting libraries for GPTs, including ChatGPT. Some of these are on Github and others are hosted externally.

## [Awesome Prompt Engineering](https://github.com/danielrosehill/Awesome-Prompt-Engineering)
This repository contains a hand-curated resources for Prompt Engineering with a focus on Generative Pre-trained Transformer (GPT), ChatGPT, PaLM etc 

## [Awesome Chatgpt Prompts](https://github.com/danielrosehill/awesome-chatgpt-prompts)
This repo includes ChatGPT prompt curation to use ChatGPT better.

## [Prompt Library](https://github.com/danielrosehill/Prompt-Library)
A repository of prompts I've used when working with LLMs as well as some example outputs and notes

