# Experiments Repositories

## [Two Ais Talk](https://github.com/danielrosehill/Two-AIs-Talk)
Just a ridiculous experiment I have to try out at least once

## [Artificial Intelligence Interview Workflow V2](https://github.com/danielrosehill/AI-Interview-Workflow-V2)
No description provided

## [Prompt Puncutation Experiment](https://github.com/danielrosehill/Prompt-Puncutation-Experiment)
Experimenting to test the effect of puncutation in prompts on inference quality

## [Bias Censorship Eval Tests](https://github.com/danielrosehill/Bias-Censorship-Eval-Tests)
Some loosely organised experiments intended to probe the levels of political censorship inherent in LLMs

## [Large Language Model Long Codegen Test](https://github.com/danielrosehill/LLM-Long-Codegen-Test)
Experiment to evaluate the ability of code-gen LLMs to generate a long continuous (single) output

## [Artificial Intelligence Generated Project Starters](https://github.com/danielrosehill/AI-Generated-Project-Starters)
AI-generated project starters for anyone wishing to carry on the ideas!

## [Fun Large Language Model Data Prompts](https://github.com/danielrosehill/Fun-LLM-Data-Prompts)
As prompting fro data analysis & vis is quite specific, a small side collection of prompts for this purpose

## [Testing Repo Public](https://github.com/danielrosehill/Testing-Repo-Public)
General first entry testing repository for things that need to be publicly accessible. 

## [Large Language Model Output Notes](https://github.com/danielrosehill/LLM-Output-Notes)
First entry documentation repository for notes mostly unedited from LLMs

## [Llms On Llms](https://github.com/danielrosehill/LLMs-on-LLMs)
Large language models (LLMs) explaining themselves - and how to make best use of them (prompt engineering). Often insightful, though accuracy not guaranteed!

## [Large Language Model Experiment Notebook](https://github.com/danielrosehill/LLM-Experiment-Notebook)
Experiments in evaluating various prompting strategies and LLM performance generally

## [Large Language Model Evaluation Prompts](https://github.com/danielrosehill/LLM-Evaluation-Prompts)
A few prompts that I am storing in a repo for the purpose of running controlled experiments comparing and benchmarking different LLMs for defined use-cases

## [Example Large Language Model Python Gui Build](https://github.com/danielrosehill/Example-LLM-Python-GUI-Build)
No description provided

## [Artificial Intelligence Generated Code Samples](https://github.com/danielrosehill/AI-Generated-Code-Samples)
Scripts and other programs generated with various LLMs. Sometimes they need refinement. Frequently they just work!

## [Large Language Model Outputs](https://github.com/danielrosehill/LLM-Outputs)
Interesting GPT outputs demonstrating specific capabilities

